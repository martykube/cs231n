{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure out spatial batch norm forward and backwards calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Batch Normalization\n",
    "We already saw that batch normalization is a very useful technique for training deep fully-connected networks. Batch normalization can also be used for convolutional networks, but we need to tweak it a bit; the modification will be called \"spatial batch normalization.\"\n",
    "\n",
    "Normally batch-normalization accepts inputs of shape `(N, D)` and produces outputs of shape `(N, D)`, where we normalize across the minibatch dimension `N`. For data coming from convolutional layers, batch normalization needs to accept inputs of shape `(N, C, H, W)` and produce outputs of shape `(N, C, H, W)` where the `N` dimension gives the minibatch size and the `(H, W)` dimensions give the spatial size of the feature map.\n",
    "\n",
    "If the feature map was produced using convolutions, then we expect the statistics of each feature channel to be relatively consistent both between different imagesand different locations within the same image. Therefore spatial batch normalization computes a mean and variance for each of the `C` feature channels by computing statistics over both the minibatch dimension `N` and the spatial dimensions `H` and `W`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial batch normalization: forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cs231n.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " err 2.16160744521e-07\n"
     ]
    }
   ],
   "source": [
    "from cs231n.layers import batchnorm_forward\n",
    "\n",
    "\n",
    "def spatial_batchnorm_forward(x, gamma, beta, bn_param):\n",
    "    N, C, H, W = x.shape\n",
    "    x = np.transpose(x, (0, 2, 3, 1))\n",
    "    x = x.reshape((N * H * W, C))\n",
    "    out, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "    out = out.reshape((N, H, W, C))\n",
    "    out = np.transpose(out, (0, 3, 1, 2))\n",
    "    return (out, cache)\n",
    "    \n",
    "    \n",
    "\n",
    "# Data\n",
    "N, C, H, W = 2, 3, 4, 5\n",
    "x = 4 * np.random.randn(N, C, H, W) + 10\n",
    "gamma, beta = np.ones(C), np.zeros(C)\n",
    "eps = 1e-7\n",
    "bn_param = {\n",
    "    'mode': 'train'\n",
    "}\n",
    "\n",
    "a = spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "b = quick_spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "\n",
    "print \"err\", rel_error(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spatial_batchnorm_forward_by_hand(x, gamma, beta, bn_param):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for spatial batch normalization.                                  \n",
    "\n",
    "    Inputs:                                                                                     \n",
    "    - x: Input data of shape (N, C, H, W)                                                       \n",
    "    - gamma: Scale parameter, of shape (C,)                                                     \n",
    "    - beta: Shift parameter, of shape (C,)                                                      \n",
    "    - bn_param: Dictionary with the following keys:                                             \n",
    "    - mode: 'train' or 'test'; required                                                       \n",
    "    - eps: Constant for numeric stability                                                     \n",
    "    - momentum: Constant for running mean / variance. momentum=0 means that                   \n",
    "      old information is discarded completely at every time step, while                       \n",
    "      momentum=1 means that new information is never incorporated. The                        \n",
    "      default of momentum=0.9 should work well in most situations.                            \n",
    "    - running_mean: Array of shape (D,) giving running mean of features                       \n",
    "    - running_var Array of shape (D,) giving running variance of features                     \n",
    "\n",
    "    Returns a tuple of:                                                                         \n",
    "    - out: Output data, of shape (N, C, H, W)                                                   \n",
    "    - cache: Values needed for the backward pass                                                \n",
    "    \"\"\"\n",
    "    eps = bn_param.get('eps', 1e-7)\n",
    "\n",
    "    # Collect per channel stats\n",
    "    mean = np.mean(x, axis=(0, 2, 3))\n",
    "    var =   np.var(x, axis=(0, 2, 3))\n",
    "\n",
    "    # Get ready to broadcast per channel stats(C) to x(N, C, H, W)\n",
    "    C = x.shape[1]\n",
    "    mean =   mean.reshape(1, C, 1, 1)\n",
    "    var =     var.reshape(1, C, 1, 1)\n",
    "    gamma = gamma.reshape(1, C, 1, 1)\n",
    "    beta =   beta.reshape(1, C, 1, 1)\n",
    "    \n",
    "    xhat = (x - mean) / np.sqrt(var + eps)\n",
    "    y = gamma * xhat + beta\n",
    "    \n",
    "    return (y, ())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx rel_error 9.2792051324e-07\n",
      "dgamma rel_error 1.67667433096e-11\n",
      "dbeta rel_error 7.91093235756e-13\n"
     ]
    }
   ],
   "source": [
    "def sbf(x, gamma, beta, bn_param):\n",
    "    eps = bn_param.get('eps', 1e-7)\n",
    "    mean = np.mean(x, axis=(0, 2, 3))\n",
    "    var =   np.var(x, axis=(0, 2, 3))\n",
    "    C = x.shape[1]\n",
    "    x_minus_mean = x - mean.reshape(1, C, 1, 1)\n",
    "    x_minus_mean_sqr = x_minus_mean * x_minus_mean\n",
    "    var = np.mean(x_minus_mean_sqr, axis=(0, 2, 3))\n",
    "    sqrt_var = (var + eps) ** (0.5)\n",
    "    one_over_sqrt_var = 1. / sqrt_var\n",
    "    xhat = x_minus_mean * one_over_sqrt_var.reshape(1, C, 1, 1)\n",
    "    y = gamma.reshape(1, C, 1, 1) * xhat + beta.reshape(1, C, 1, 1)\n",
    "    return y\n",
    "\n",
    "# Numerical gradient\n",
    "# dout = 5 * np.random.randn(C)\n",
    "dout = 5 * np.random.randn(N, C, H, W)\n",
    "fx = lambda x: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fgamma = lambda gamma: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fbeta = lambda beta: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "dgamma_num = eval_numerical_gradient_array(fgamma, gamma, dout)\n",
    "dbeta_num = eval_numerical_gradient_array(fbeta, beta, dout)\n",
    "\n",
    "# Calculated gradient\n",
    "\n",
    "# forward\n",
    "mean = np.mean(x, axis=(0, 2, 3))\n",
    "x_minus_mean = x - mean.reshape(1, C, 1, 1)\n",
    "x_minus_mean_sqr = x_minus_mean * x_minus_mean\n",
    "var = np.mean(x_minus_mean_sqr, axis=(0, 2, 3))\n",
    "sqrt_var = np.sqrt(var + eps)\n",
    "one_over_sqrt_var = 1. / sqrt_var\n",
    "xhat = x_minus_mean * one_over_sqrt_var.reshape(1, C, 1, 1)\n",
    "y = gamma.reshape(1, C, 1, 1) * xhat + beta.reshape(1, C, 1, 1)\n",
    "\n",
    "# back prop across beta and gamma to dxhat\n",
    "\n",
    "# 9 + beta                                                                                  \n",
    "dbeta = np.sum(dout, axis=(0, 2, 3))\n",
    "dgamma_xhat = dout\n",
    "\n",
    "# 8 xhat * gamma                                                                            \n",
    "dgamma = np.sum(dgamma_xhat * xhat, axis=(0, 2, 3))\n",
    "dxhat = dgamma_xhat * gamma.reshape(1, C, 1, 1)\n",
    "\n",
    "# 7) backprop dxhat to done_over_sqrt_var and dx_minus_mean\n",
    "done_over_sqrt_var = np.sum(x_minus_mean * dxhat, axis=(0, 2, 3))\n",
    "dx_minus_mean_top = dxhat * one_over_sqrt_var.reshape(1, C, 1, 1)\n",
    "\n",
    "# 6) Backprop 1/x \n",
    "dsqrt_var = -(sqrt_var ** (-2)) * done_over_sqrt_var\n",
    "\n",
    "# 5) backprop sqrt(var + eps)\n",
    "dvar = 0.5 * ((var + eps) ** (-0.5)) * dsqrt_var\n",
    "\n",
    "# 4) Back prop mean(x - mean)\n",
    "dx_minus_mean_sqr = (1. / (N * H * W)) * np.ones((N, C, H, W)) * dvar.reshape(1, C, 1, 1)\n",
    "\n",
    "# 3) Back prop x^2\n",
    "dx_minus_mean_bottom = 2 * x_minus_mean * dx_minus_mean_sqr\n",
    "\n",
    "# 2) backprop dx_minus_mean to input x\n",
    "# colect both inbound gradient \n",
    "dx_minus_mean = dx_minus_mean_bottom + dx_minus_mean_top\n",
    "dx_input = dx_minus_mean\n",
    "dx_mean = -np.sum(dx_minus_mean, axis=(0, 2, 3))\n",
    "\n",
    "# 1) back prop dmean to dx\n",
    "dx_via_mean = (1. / (N * H * W)) * np.ones((N, C, H, W)) * dx_mean.reshape(1, C, 1, 1)\n",
    "\n",
    "dx = dx_input + dx_via_mean\n",
    "\n",
    "print \"dx rel_error\", rel_error(dx, dx_num)\n",
    "print \"dgamma rel_error\", rel_error(dgamma, dgamma_num)\n",
    "print \"dbeta rel_error\", rel_error(dbeta, dbeta_num)\n",
    "\n",
    "# print \"dx_num\\n\", dx_num\n",
    "# print \"dx\\n\", dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before spatial batch normalization:\n",
      "  Shape:  (2, 3, 4, 5)\n",
      "  Means:  [ 10.05246971   9.25304674   8.73905878]\n",
      "  Stds:  [ 5.02652078  3.73102912  4.3636944 ]\n",
      "After spatial batch normalization:\n",
      "  Shape:  (2, 3, 4, 5)\n",
      "  Means:  [ -1.02695630e-16   2.22044605e-16   3.05311332e-17]\n",
      "  Stds:  [ 1.  1.  1.]\n",
      "After spatial batch normalization (nontrivial gamma, beta):\n",
      "  Shape:  (2, 3, 4, 5)\n",
      "  Means:  [ 6.  7.  8.]\n",
      "  Stds:  [ 2.99999999  3.99999999  4.99999999]\n"
     ]
    }
   ],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after spatial batch normalization\n",
    "\n",
    "N, C, H, W = 2, 3, 4, 5\n",
    "x = 4 * np.random.randn(N, C, H, W) + 10\n",
    "x = x.astype(np.float64)\n",
    "\n",
    "print 'Before spatial batch normalization:'\n",
    "print '  Shape: ', x.shape\n",
    "print '  Means: ', x.mean(axis=(0, 2, 3))\n",
    "print '  Stds: ', x.std(axis=(0, 2, 3))\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "gamma, beta = np.ones(C), np.zeros(C)\n",
    "bn_param = {'mode': 'train'}\n",
    "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "print 'After spatial batch normalization:'\n",
    "print '  Shape: ', out.shape\n",
    "print '  Means: ', out.mean(axis=(0, 2, 3))\n",
    "print '  Stds: ', out.std(axis=(0, 2, 3))\n",
    "\n",
    "# Means should be close to beta and stds close to gamma\n",
    "gamma, beta = np.asarray([3, 4, 5]), np.asarray([6, 7, 8])\n",
    "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "print 'After spatial batch normalization (nontrivial gamma, beta):'\n",
    "print '  Shape: ', out.shape\n",
    "print '  Means: ', out.mean(axis=(0, 2, 3))\n",
    "print '  Stds: ', out.std(axis=(0, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
